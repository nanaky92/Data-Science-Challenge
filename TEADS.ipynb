{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teads challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   creative_id user_operating_system       user_device  \\\n",
      "0       113521               Android             Phone   \n",
      "1       115340               Windows  PersonalComputer   \n",
      "2       113582               Android             Phone   \n",
      "3        97385               Windows  PersonalComputer   \n",
      "4       114821               Windows  PersonalComputer   \n",
      "5       113065               Android            Tablet   \n",
      "6       111414                   iOS             Phone   \n",
      "7       111414                   iOS             Phone   \n",
      "8       112705               Windows  PersonalComputer   \n",
      "9       113176                 macOS  PersonalComputer   \n",
      "\n",
      "   average_seconds_played      cost  revenue  \n",
      "0                     NaN  0.010128      0.0  \n",
      "1                0.000000  0.005937      0.0  \n",
      "2                7.142857  0.004398      0.0  \n",
      "3                     NaN  0.006157      0.0  \n",
      "4                     NaN  0.001994      0.0  \n",
      "5                     NaN  0.003781      0.0  \n",
      "6               17.000000  0.002836      0.0  \n",
      "7                     NaN  0.002836      0.0  \n",
      "8               14.560606  0.006157      0.0  \n",
      "9                0.000000  0.006157      0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"aft100k.csv\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preeliminary questions\n",
    "\n",
    "####  1) The margin being defined as (revenue - cost) / revenue, what is our global margin based on this log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there null revenues? False\n",
      "Are there null costs? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Are there null revenues? {}\".format(df[\"revenue\"].isnull().values.any()))\n",
    "print(\"Are there null costs? {}\".format(df[\"cost\"].isnull().values.any()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global margin is 0.2719, or 27.19%\n"
     ]
    }
   ],
   "source": [
    "revs = df[\"revenue\"].sum()\n",
    "costs = df[\"cost\"].sum()\n",
    "margin = (revs-costs)/revs\n",
    "print(\"Global margin is {0:.4f}, or {1:.2f}%\".format(margin, margin*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Can you think of other interesting metrics to optimize\n",
    "\n",
    "Not in the csv: Click-through rate (CTR), Conversion rate.\n",
    "\n",
    "I would also like to understand why sometimes it takes more seconds than others to get the money - some of them don't make us any money in 30 seconds, some of them do in 10 seconds. I guess it is very possible that we are losing money in some type of ads. Maybe we should only display them in certain platforms where people is willing to spend more time watching ads - and this is a metric we could get from our csv, the number of seconds per device.\n",
    "\n",
    "Metrics to optimize, from the csv:\n",
    "1) Profits - No explanation needed.\n",
    "2) Profits per device - We want to make sure all devices are profitable.\n",
    "3) Profits per OS - We want to make sure all OSs are profitable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) What are the most profitable Operating Systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_operating_system\n",
      "iOS    115.521775\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_agg = df[['user_operating_system','cost', 'revenue']].groupby(\n",
    "    ['user_operating_system']).sum()\n",
    "series_profits = df_agg[\"revenue\"]-df_agg[\"cost\"]\n",
    "print(series_profits[series_profits==max(series_profits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning questions\n",
    "\n",
    "#### How would you use this historical data to predict the event 'we benefit from a revenue' (ie revenue > 0) in a dataset where the revenue is not known yet?\n",
    "\n",
    "#### Compute the prediction accuracy of a well chosen algorithm and comment the results. Do not hesitate to describe your methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem at hand is a problem of supervised learning, in particular, it is a regression problem. We need to build a regressor from our dataset, using:\n",
    "\n",
    "- Four features: *'user_operating_system', 'user_device', 'average_seconds_played', 'cost'*. \n",
    "- A label: *revenues*.\n",
    "- A regression algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning:\n",
    "\n",
    "We have a dataset with 100 000 examples - that is pretty good to use some of the more complex algorithms like SVMs with kernels, or ensemble learning (random forests, adaboost). Neural networks could also be considered but, considering the scope of this work, it would take too long to train and optimize.\n",
    "\n",
    "The first problem we face is that the features user_device and average_seconds_played are not present in some examples of the data set - or, in pandas' jargon, they are NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_operating_system         0\n",
       "user_device                   8\n",
       "average_seconds_played    61828\n",
       "cost                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = ['user_operating_system', 'user_device', \n",
    "                'average_seconds_played', 'cost']\n",
    "X = df.loc[:, feature_cols]\n",
    "\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of *user_device*, it is not a big problem (only 8 labels out of 100 000, we can discard 8 without any second thoughts). Also substituting the NaNs for the average or something similar is tricky since this is a categorical feature (i.e., a string) - we could use the most frequent value but, again, there is no need. \n",
    "\n",
    "In the case of *average_seconds_played*, the problem is different: more than half of the data does not contain a value for the feature. However in this case, this is a numerical feature which gives us two options:\n",
    "\n",
    "1) Removal: We would end up with 39% of the dataset... do we really want to do this? Probably not, but hey, let's try and let the data talk.\n",
    "\n",
    "2) Substituting the data with its average, mode or median. This looks more promising, and it will be our default approach.\n",
    "\n",
    "##### Feature Selection and Scaling\n",
    "\n",
    "Feature generation and feature selection is one of the most important time consumn\n",
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-7-05ae2b97a5af>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-05ae2b97a5af>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    'average_seconds_played', 'cost', 'revenue'\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " 'user_operating_system', 'user_device',\n",
    "       'average_seconds_played', 'cost', 'revenue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = svm.SVR()\n",
    "clf.fit(X, y) \n",
    "\n",
    "\n",
    "clf.predict([[1, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we don't have another dataset, let's use the given dataset as a proxy for the new dataset - although, of course, we will use the revenue not as a feature but as a label. \n",
    "\n",
    "Use an SVR and ensemble regressors \n",
    "Feature selection? Evidently, no creative id\n",
    "\n",
    "Cross Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
